{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcca87b",
   "metadata": {},
   "source": [
    "## Introducción general\n",
    "\n",
    "En este trabajo se desarrolla una exploración sobre el diseño y la optimización de prompts para un asistente de IA aplicado a la gestión institucional de IOMA.  \n",
    "El objetivo es mostrar cómo, a partir de un mismo caso de uso, la calidad, precisión y costo de las respuestas del modelo dependen directamente del nivel de detalle y de las técnicas empleadas en la construcción del prompt.\n",
    "\n",
    "Para ello se parte de una **base de conocimiento real** (FAQs institucionales) y de una **misma consulta representativa**.  \n",
    "A lo largo del cuaderno se presentan distintas versiones de prompts, organizadas de manera progresiva:\n",
    "\n",
    "- **Versión 1**: prompt mínimo, casi sin reglas.  \n",
    "- **Versión 2**: incorporación de rol, público y tono.  \n",
    "- **Versión 3**: primeras reglas y estructura básica.  \n",
    "- **Versión 4**: estructura más completa con subsecciones y límite de extensión, cerrada exclusivamente a la BASE.  \n",
    "- **Versión 5**: prompt mixto, donde el checklist se mantiene atado a la BASE, pero los términos clave y las buenas prácticas pueden enriquecerse con conocimiento externo.  \n",
    "- **Versión 6**: optimización orientada a **reducir costos**, aplicando técnicas de *Fast Prompting* (BASE compacta, reglas breves y salida en JSON conciso).\n",
    "\n",
    "Cada versión se acompaña con su correspondiente explicación, ejecución práctica y cálculo de costos, lo que permite comparar la evolución en tres ejes:  \n",
    "1. **Calidad y claridad de la respuesta**.  \n",
    "2. **Grado de cumplimiento de las reglas definidas** (uso exclusivo de la BASE o apertura a conocimiento externo).  \n",
    "3. **Uso de tokens y costo asociado** en cada ejecución.\n",
    "\n",
    "De este modo, se evidencia cómo el diseño de prompts es un proceso iterativo que combina creatividad, precisión y eficiencia, y cómo la aplicación de técnicas de *Fast Prompting* no solo mejora la confiabilidad de las respuestas, sino que también optimiza la **eficiencia económica** del sistema.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c27a1c8",
   "metadata": {},
   "source": [
    "## Preparación y funciones auxiliares\n",
    "\n",
    "Antes de ejecutar los distintos prompts es necesario preparar la base de conocimiento y definir un conjunto de funciones que permitan seleccionar la FAQ más relevante según la consulta realizada.  \n",
    "Este bloque cumple tres objetivos principales:\n",
    "\n",
    "1. **Carga y filtrado de la base**  \n",
    "   - Se importa el archivo CSV con las FAQs.  \n",
    "   - Se filtran únicamente las que tienen estado \"vigente\" o \"en revisión\".  \n",
    "   - Se normalizan columnas clave para evitar valores nulos o inconsistencias.\n",
    "\n",
    "2. **Tokenización y relevancia**  \n",
    "   - `dividir_en_tokens(texto)`: transforma un texto en una lista de tokens en minúscula, eliminando signos de puntuación.  \n",
    "   - `calcular_relevancia(tokens_consulta, fila)`: asigna un puntaje a cada fila de la base comparando los tokens de la consulta con las palabras clave, el título y el contenido. Se da más peso a las coincidencias en palabras clave (×3), luego en título (×2) y por último en contenido (×1).\n",
    "\n",
    "3. **Selección de FAQ**  \n",
    "   - `buscar_faq(consulta, tabla)`: utiliza las funciones anteriores para identificar qué fila de la base tiene mayor relevancia frente a la consulta ingresada.  \n",
    "   - Si encuentra coincidencias, devuelve la FAQ más relevante; de lo contrario, indica que no hay resultados y que corresponde derivar el caso.\n",
    "\n",
    "En síntesis, estas funciones permiten **conectar de forma eficiente la pregunta de un agente con la información normativa más pertinente**, asegurando que cada prompt se construya sobre la BASE adecuada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0943f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from textwrap import dedent\n",
    "import csv\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Cargar base de conocimiento\n",
    "df = pd.read_csv(\"../data/base_conocimiento_afiliaciones_clean.csv\", dtype=str, keep_default_na=False)\n",
    "\n",
    "# Filtrar por estado\n",
    "df = df[df[\"estado\"].str.lower().isin([\"vigente\",\"en revisión\"])].copy()\n",
    "\n",
    "# Normalizar columnas\n",
    "for col in [\"id\",\"titulo\",\"contenido\",\"respuesta_validada\",\"palabras_clave\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(\"\").astype(str)\n",
    "\n",
    "# Funciones auxiliares\n",
    "def dividir_en_tokens(texto: str):\n",
    "    texto = texto.lower()\n",
    "    return [t for t in re.split(r\"[^a-záéíóúñü0-9]+\", texto) if t]\n",
    "\n",
    "def calcular_relevancia(tokens_consulta, fila):\n",
    "    puntaje = 0\n",
    "    puntaje += 3 * len(set(tokens_consulta) & set(dividir_en_tokens(fila.get(\"palabras_clave\",\"\"))))\n",
    "    puntaje += 2 * len(set(tokens_consulta) & set(dividir_en_tokens(fila.get(\"titulo\",\"\"))))\n",
    "    puntaje += 1 * len(set(tokens_consulta) & set(dividir_en_tokens(fila.get(\"contenido\",\"\"))))\n",
    "    return puntaje\n",
    "\n",
    "def buscar_faq(consulta: str, tabla: pd.DataFrame):\n",
    "    toks_consulta = dividir_en_tokens(consulta)\n",
    "    puntuadas = [(calcular_relevancia(toks_consulta, fila), idx) for idx, fila in tabla.iterrows()]\n",
    "    puntuadas = [(p,i) for p,i in puntuadas if p>0]\n",
    "    if not puntuadas:\n",
    "        return None\n",
    "    puntuadas.sort(reverse=True)\n",
    "    return tabla.loc[puntuadas[0][1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a97bf50",
   "metadata": {},
   "source": [
    "### Configuración inicial\n",
    "En este bloque se establece la configuración general necesaria para trabajar con la API de OpenAI.  \n",
    "Se cargan las variables de entorno con la clave de acceso, se inicializa el cliente y se define la consulta de prueba que utilizaremos a lo largo de todas las ejecuciones.  \n",
    "Además, se selecciona de la base de conocimiento la FAQ más relevante frente a la consulta, sobre la cual se construirán los distintos prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe4376f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ seleccionada: faq_004 - Afiliación de recién nacido/a\n"
     ]
    }
   ],
   "source": [
    "# Configuración del cliente\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Consulta de prueba\n",
    "consulta = \"¿Qué documentación necesito para afiliar a un recien nacido?\"\n",
    "\n",
    "# Selección de fila relevante\n",
    "fila_sel = buscar_faq(consulta, df)\n",
    "if fila_sel is None:\n",
    "    raise ValueError(\"No se encontró una FAQ relevante. Derivar a Afiliaciones (SLA 24 h).\")\n",
    "\n",
    "print(\"FAQ seleccionada:\", fila_sel.get(\"id\",\"(sin id)\"), \"-\", fila_sel.get(\"titulo\",\"(sin título)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf5307",
   "metadata": {},
   "source": [
    "### Bloque 2 – Función genérica de ejecución\n",
    "Aquí se define una función auxiliar `ejecutar_prompt`, que permite reutilizar el mismo flujo de implementación con cualquier versión del prompt.  \n",
    "La función recibe como parámetros:\n",
    "- la función `construir_prompt_vX` que define la variante del prompt,  \n",
    "- la fila seleccionada de la base,  \n",
    "- la consulta planteada.  \n",
    "\n",
    "De este modo evitamos repetir código en cada prueba y podemos ejecutar fácilmente todas las versiones de prompt simplemente cambiando la función que se pasa como argumento.  \n",
    "\n",
    "Además, dentro de la misma función se incorporó el **cálculo automático de costos**, a partir de los tokens de entrada (prompt) y de salida (respuesta) que devuelve la API.  \n",
    "Esto permite obtener en cada ejecución:\n",
    "- Cantidad de tokens consumidos en el prompt.  \n",
    "- Cantidad de tokens generados en la respuesta.  \n",
    "- Total de tokens procesados.  \n",
    "- **Costo estimado en dólares** de esa llamada al modelo.  \n",
    "\n",
    "De esta manera, no solo se evalúa la calidad de las respuestas según cada versión de prompt, sino también su **eficiencia económica**, aspecto central en la optimización con *Fast Prompting*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3790be4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Precios por millón de tokens  ---\n",
    "PRECIOS_USD = {\n",
    "    \"gpt-4o-mini\": {\"in\": 0.15, \"out\": 0.60}\n",
    "}\n",
    "\n",
    "def ejecutar_prompt(fn_prompt, fila_sel, consulta,\n",
    "                    model=\"gpt-4o-mini\", temperature=0.3, max_tokens=400):\n",
    "    \"\"\"\n",
    "    fn_prompt: función construir_prompt_vX (ej. construir_prompt_v5)\n",
    "    fila_sel:  fila seleccionada de la base\n",
    "    consulta:  string con la pregunta del agente\n",
    "\n",
    "    Muestra la respuesta del modelo + tokens consumidos y costo estimado (USD).\n",
    "    \"\"\"\n",
    "    # 1) Construcción del prompt (system = reglas + BASE; user = consulta)\n",
    "    prompt = fn_prompt(fila_sel, consulta)\n",
    "\n",
    "    # 2) Llamada al modelo\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": consulta},\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "\n",
    "    # 3) Mostrar respuesta (evitamos eco duplicado retornando al final)\n",
    "    respuesta = resp.choices[0].message.content\n",
    "    print(\"\\n--- RESPUESTA DEL MODELO ---\\n\")\n",
    "    print(respuesta)\n",
    "\n",
    "    # 4) Cálculo de costos con usage\n",
    "    precios = PRECIOS_USD.get(model, PRECIOS_USD[\"gpt-4o-mini\"])\n",
    "    pt = getattr(resp.usage, \"prompt_tokens\", 0)\n",
    "    ct = getattr(resp.usage, \"completion_tokens\", 0)\n",
    "    tt = getattr(resp.usage, \"total_tokens\", pt + ct)\n",
    "\n",
    "    costo_in  = (pt / 1_000_000) * precios[\"in\"]\n",
    "    costo_out = (ct / 1_000_000) * precios[\"out\"]\n",
    "    costo_tot = costo_in + costo_out\n",
    "\n",
    "    print(\"\\n--- COSTOS ---\")\n",
    "    print(f\"Modelo: {model} | Temp: {temperature} | Máx. tokens salida: {max_tokens}\")\n",
    "    print(f\"Prompt tokens: {pt} | Completion tokens: {ct} | Total: {tt}\")\n",
    "    print(f\"Costo entrada (USD): {costo_in:.8f}\")\n",
    "    print(f\"Costo salida  (USD): {costo_out:.8f}\")\n",
    "    print(f\"Costo total (USD): {costo_tot:.8f}\")\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2efa4d",
   "metadata": {},
   "source": [
    "### Versión 1 – Prompt básico (mínimo)\n",
    "Comenzamos con un prompt muy simple, casi sin reglas.  \n",
    "El asistente solo recibe la pregunta y la base como contexto, y se le pide responder.  \n",
    "No hay estructura definida ni control sobre la extensión o la fidelidad de la respuesta.  \n",
    "Esta versión nos sirve como **punto de partida** para observar cómo responde el modelo con indicaciones mínimas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df3b251e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESPUESTA DEL MODELO ---\n",
      "\n",
      "Para afiliar a un recién nacido, necesitarás presentar la documentación correspondiente en la Delegación de IOMA que corresponda al domicilio de la persona afiliada titular. También puedes iniciar el trámite a través de la App IOMA Digital/Autogestión. Asegúrate de tener a mano el documento de identidad del recién nacido y cualquier otro documento que pueda ser requerido.\n",
      "\n",
      "--- COSTOS ---\n",
      "Modelo: gpt-4o-mini | Temp: 0.3 | Máx. tokens salida: 400\n",
      "Prompt tokens: 86 | Completion tokens: 76 | Total: 162\n",
      "Costo entrada (USD): 0.00001290\n",
      "Costo salida  (USD): 0.00004560\n",
      "Costo total (USD): 0.00005850\n"
     ]
    }
   ],
   "source": [
    "# VERSIÓN 1 - MUY BÁSICO (casi sin reglas)\n",
    "def construir_prompt_v1(fila, pregunta: str):\n",
    "    base = (fila.get(\"contenido\") or fila.get(\"titulo\",\"\")).strip()\n",
    "    return f\"\"\"\n",
    "    Pregunta: {pregunta}\n",
    "    Base: {base}\n",
    "    Responde usando la base.\n",
    "    \"\"\"\n",
    "ejecutar_prompt(construir_prompt_v1, fila_sel, consulta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079090d",
   "metadata": {},
   "source": [
    "### Versión 2 – Rol y tono\n",
    "\n",
    "En esta segunda versión agregamos información contextual:  \n",
    "- Definimos un **rol** (“Asistente de Afiliaciones de IOMA”).  \n",
    "- Establecemos el **público** (agentes administrativos).  \n",
    "- Indicamos un **tono** (claro, formal e institucional).  \n",
    "\n",
    "Esto corresponde a una de las técnicas de *Fast Prompting*: **dar contexto explícito al modelo**.  \n",
    "Al asignar un rol, un público objetivo y un tono, se reduce la ambigüedad y se orienta la respuesta hacia el estilo esperado, evitando desvíos en la redacción.  \n",
    "\n",
    "De esta manera empezamos a darle al modelo un marco más controlado, aunque todavía sin imponer una estructura rígida en la salida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb543d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESPUESTA DEL MODELO ---\n",
      "\n",
      "Para afiliar a un recién nacido, se requiere presentar la documentación correspondiente en la Delegación de IOMA que corresponda al domicilio del afiliado titular, o a través de la App IOMA Digital/Autogestión.\n",
      "\n",
      "--- COSTOS ---\n",
      "Modelo: gpt-4o-mini | Temp: 0.3 | Máx. tokens salida: 400\n",
      "Prompt tokens: 124 | Completion tokens: 46 | Total: 170\n",
      "Costo entrada (USD): 0.00001860\n",
      "Costo salida  (USD): 0.00002760\n",
      "Costo total (USD): 0.00004620\n"
     ]
    }
   ],
   "source": [
    "# VERSIÓN 2 - CON ROL Y TONO\n",
    "def construir_prompt_v2(fila, pregunta: str):\n",
    "    base = (fila.get(\"contenido\") or fila.get(\"titulo\",\"\")).strip()\n",
    "    return f\"\"\"\n",
    "    Rol: Asistente de Afiliaciones de IOMA\n",
    "    Público: agentes administrativos\n",
    "    Tono: claro, formal, institucional\n",
    "\n",
    "    Pregunta: {pregunta}\n",
    "\n",
    "    Base de referencia:\n",
    "    {base}\n",
    "\n",
    "    Responde de forma clara y breve usando SOLO la base.\n",
    "    \"\"\"\n",
    "ejecutar_prompt(construir_prompt_v2, fila_sel, consulta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c107880",
   "metadata": {},
   "source": [
    "### Versión 3 – Estructura básica y reglas\n",
    "\n",
    "Aquí damos un salto importante:  \n",
    "- Se aclara que el modelo debe usar **exclusivamente la BASE** como fuente de información.  \n",
    "- Se introduce la regla de que, si falta un dato, debe indicarlo explícitamente con un mensaje predefinido.  \n",
    "- Se exige una **estructura mínima de salida**, que incluye un checklist de pasos y un cierre obligatorio con la frase exacta.  \n",
    "\n",
    "En esta versión se aplican técnicas de *Fast Prompting* más avanzadas que en la anterior:  \n",
    "- **Grounding estricto en la BASE**, para reducir invención o “alucinación” del modelo.  \n",
    "- **Instrucciones de fallback** (qué hacer si falta información), lo que garantiza consistencia en todos los casos.  \n",
    "- **Estructuración de la salida**, que disminuye la ambigüedad y facilita la validación posterior.  \n",
    "\n",
    "De este modo, el prompt pasa de ser meramente contextual (Versión 2) a estar controlado por **reglas claras y un formato mínimo esperado**, aumentando la confiabilidad de la respuesta.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf393d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERSIÓN 3 - CON ESTRUCTURA Y REGLAS\n",
    "def construir_prompt_v3(fila, pregunta: str):\n",
    "    base = (fila.get(\"contenido\") or fila.get(\"titulo\",\"\")).strip()\n",
    "    return f\"\"\"\n",
    "    Rol: Asistente de Afiliaciones de IOMA\n",
    "    Público: agentes en Delegaciones\n",
    "    Tono: institucional, claro y preciso\n",
    "\n",
    "    Reglas:\n",
    "    - Basate EXCLUSIVAMENTE en la Base.\n",
    "    - Si falta un dato, decí \"No consta en la normativa adjunta\".\n",
    "\n",
    "    Respuesta debe incluir:\n",
    "    1. Pasos o checklist (en viñetas).\n",
    "    2. Cierre con: \"Fuente: base de conocimiento vigente\".\n",
    "\n",
    "    Pregunta: {pregunta}\n",
    "\n",
    "    BASE:\n",
    "    {base}\n",
    "    \"\"\"\n",
    "\n",
    "ejecutar_prompt(construir_prompt_v3, fila_sel, consulta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12247a6",
   "metadata": {},
   "source": [
    "### Versión 4 – Prompt más detallado con subsecciones (cerrado a la BASE)\n",
    "\n",
    "En esta versión el prompt se vuelve más completo y estructurado:  \n",
    "\n",
    "- Se fija un **límite máximo de extensión** (≤ 350 palabras).  \n",
    "- Se exige un **cierre exacto** con la frase “Fuente: base de conocimiento vigente”.  \n",
    "- Se introduce un **contexto ampliado** con dos subsecciones:  \n",
    "  - **Términos clave** (≥3 ítems).  \n",
    "  - **Objetivo y buenas prácticas** (≥3 ítems).  \n",
    "\n",
    "La técnica de *Fast Prompting* aplicada aquí es el **prompt estructurado con subsecciones y restricciones de formato**.  \n",
    "Esto aporta varios beneficios:  \n",
    "- **Control de longitud** → evitar respuestas excesivas y optimizar el uso de tokens.  \n",
    "- **Formato obligatorio con subsecciones** → fuerza al modelo a organizar la salida en bloques predefinidos.  \n",
    "- **Grounding cerrado en la BASE** → asegura que todo el contenido provenga exclusivamente de la normativa institucional, sin invenciones externas.  \n",
    "\n",
    "De esta manera, se logra un alto grado de control sobre la respuesta, a costa de un mayor consumo de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeae25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESPUESTA DEL MODELO ---\n",
      "\n",
      "**Checklist de documentación requerida**\n",
      "- Credencial o DNI del afiliado/a titular.\n",
      "- DNI del menor o constancia de parto donde figuren madre y padre.\n",
      "- Ficha de afiliación cumplimentada (no certificada, solo la parte del afiliado).\n",
      "- Último recibo de haberes.\n",
      "- CUIL si lo posee.\n",
      "- Certificado de nacimiento del menor.\n",
      "\n",
      "Si la afiliación se realiza sin el DNI del niño/a, deberá presentarse obligatoriamente una vez obtenido. Con esta documentación se gestiona la incorporación del recién nacido/a a cargo del afiliado/a titular.\n",
      "\n",
      "### Términos clave\n",
      "- **Afiliado/a titular**: Persona que posee la cobertura de IOMA y es responsable de la inclusión de sus dependientes.\n",
      "- **DNI**: Documento Nacional de Identidad, que acredita la identidad de una persona en Argentina.\n",
      "- **Constancia de parto**: Documento que certifica el nacimiento de un menor y puede ser utilizado en lugar del DNI hasta que este sea emitido.\n",
      "\n",
      "### Objetivo y buenas prácticas\n",
      "- **Completar la ficha de afiliación**: Asegurarse de que la ficha esté correctamente cumplimentada para evitar demoras en el proceso de afiliación.\n",
      "- **Recopilar toda la documentación necesaria**: Tener todos los documentos listos antes de iniciar el trámite facilita y acelera el proceso de afiliación.\n",
      "- **Presentar el DNI del menor a la brevedad**: Si no se cuenta con el DNI al momento de la afiliación, es importante presentarlo tan pronto como se obtenga para completar el registro.\n",
      "\n",
      "Fuente: base de conocimiento vigente.\n",
      "\n",
      "--- COSTOS ---\n",
      "Modelo: gpt-4o-mini | Temp: 0.3 | Máx. tokens salida: 400\n",
      "Prompt tokens: 422 | Completion tokens: 335 | Total: 757\n",
      "Costo entrada (USD): 0.00006330\n",
      "Costo salida  (USD): 0.00020100\n",
      "Costo total (USD): 0.00026430\n"
     ]
    }
   ],
   "source": [
    "# VERSIÓN 4 - MÁS DETALLADA (subsecciones y límites)\n",
    "def construir_prompt_v4(fila, pregunta: str):\n",
    "    base = (fila.get(\"respuesta_validada\") or \n",
    "            fila.get(\"contenido\") or \n",
    "            fila.get(\"titulo\",\"\")).strip()\n",
    "    return f\"\"\"\n",
    "    Rol: Asistente de Afiliaciones de IOMA\n",
    "    Público: agentes en Delegaciones \n",
    "    Tono: institucional, claro y preciso\n",
    "\n",
    "    Reglas:\n",
    "    - Basate EXCLUSIVAMENTE en la BASE que está entre <<BASE>> y <<FIN_BASE>>.\n",
    "    - Si un dato, definición o recomendación NO aparece en la BASE, escribí literalmente: \"No consta en la normativa adjunta\".\n",
    "    - Está prohibido inventar o usar información externa.\n",
    "    - Extensión total ≤ 350 palabras.\n",
    "    - El cierre debe ser EXACTAMENTE: \"Fuente: base de conocimiento vigente\".\n",
    "\n",
    "    La respuesta debe incluir:\n",
    "    1. Un único bloque titulado **Checklist de documentación requerida** con la lista de ítems de la BASE. \n",
    "       No repitas la misma lista fuera de esta sección.\n",
    "    2. Cierre exacto con la frase indicada.\n",
    "    3. Contexto ampliado con dos subsecciones:\n",
    "       - Términos clave: ≥3 ítems, cada uno en el formato \"Término: definición\".\n",
    "       - Objetivo y buenas prácticas: ≥3 ítems, cada uno en el formato \"Práctica: explicación\".\n",
    "\n",
    "    Pregunta: \"{pregunta}\"\n",
    "\n",
    "    <<BASE>>\n",
    "    {base}\n",
    "    <<FIN_BASE>>\n",
    "    \"\"\".strip()\n",
    "\n",
    "ejecutar_prompt(construir_prompt_v4, fila_sel, consulta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cceed3",
   "metadata": {},
   "source": [
    "### Versión 5 – Prompt mixto (BASE + conocimiento externo)\n",
    "\n",
    "En esta versión se separa explícitamente el **origen de la información** por sección:\n",
    "\n",
    "- **Checklist de documentación** → se extrae **solo** de la BASE (normativa) para garantizar exactitud y trazabilidad.  \n",
    "- **Términos clave** y **Objetivo y buenas prácticas** → el modelo puede usar **conocimiento externo** para enriquecer la explicación con definiciones y recomendaciones claras, aun cuando la normativa no las explicite.  \n",
    "\n",
    "**Técnicas de *Fast Prompting* aplicadas:**\n",
    "- **Delimitadores de contexto** (`<<BASE>> ... <<FIN_BASE>>`) → aíslan el texto normativo y reducen la ambigüedad.  \n",
    "- **Ámbito por sección (scoped prompting)** → define qué partes deben basarse en la BASE y cuáles pueden abrirse al conocimiento externo, evitando confusiones.  \n",
    "- **Estructura de salida en JSON** → asegura respuestas consistentes, fáciles de validar y de reutilizar en sistemas posteriores.  \n",
    "\n",
    "Este diseño introduce un equilibrio novedoso: mantiene el rigor normativo en los apartados críticos, pero aprovecha el conocimiento del modelo para agregar valor pedagógico en las explicaciones, mostrando cómo *Fast Prompting* permite modular el alcance de la respuesta según la sección.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25352d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESPUESTA DEL MODELO ---\n",
      "\n",
      "{\n",
      "  \"checklist\": [\n",
      "    \"Credencial o DNI del afiliado/a titular\",\n",
      "    \"DNI del menor o constancia de parto donde figuren madre y padre\",\n",
      "    \"Ficha de afiliación cumplimentada (no certificada, solo la parte del afiliado)\",\n",
      "    \"Último recibo de haberes\",\n",
      "    \"CUIL si lo posee\",\n",
      "    \"Certificado de nacimiento del menor\"\n",
      "  ],\n",
      "  \"terminos_clave\": [\n",
      "    \"DNI: Documento Nacional de Identidad, identificación oficial en Argentina.\",\n",
      "    \"CUIL: Código Único de Identificación Laboral, número que identifica a los trabajadores en el sistema de seguridad social.\",\n",
      "    \"Constancia de parto: Documento que acredita el nacimiento de un menor, indicando los datos de los padres.\"\n",
      "  ],\n",
      "  \"objetivo_y_buenas_practicas\": [\n",
      "    \"Verificar que toda la documentación esté completa antes de iniciar el proceso de afiliación.\",\n",
      "    \"Asegurarse de que la ficha de afiliación esté correctamente cumplimentada.\",\n",
      "    \"Informar al afiliado/a titular sobre la necesidad de presentar el DNI del niño/a una vez obtenido.\"\n",
      "  ],\n",
      "  \"cierre\": \"Fuente: base de conocimiento vigente\"\n",
      "}\n",
      "\n",
      "--- COSTOS ---\n",
      "Modelo: gpt-4o-mini | Temp: 0.3 | Máx. tokens salida: 400\n",
      "Prompt tokens: 482 | Completion tokens: 253 | Total: 735\n",
      "Costo entrada (USD): 0.00007230\n",
      "Costo salida  (USD): 0.00015180\n",
      "Costo total (USD): 0.00022410\n"
     ]
    }
   ],
   "source": [
    "def construir_prompt_v5(fila, pregunta: str):\n",
    "    base = (fila.get(\"respuesta_validada\") or\n",
    "            fila.get(\"contenido\") or\n",
    "            fila.get(\"titulo\",\"\")).strip()\n",
    "    return f\"\"\"\n",
    "    Rol: Asistente de Afiliaciones de IOMA\n",
    "    Público: agentes en Delegaciones\n",
    "    Tono: institucional y claro\n",
    "\n",
    "    Instrucciones por sección (ámbito de información):\n",
    "    - \"checklist\": basate EXCLUSIVAMENTE en lo que está entre <<BASE>> y <<FIN_BASE>>.\n",
    "      Si falta un dato, escribir: \"No consta en la normativa adjunta\".\n",
    "    - \"terminos_clave\" y \"objetivo_y_buenas_practicas\": podés usar tu conocimiento general externo.\n",
    "      Incluir definiciones y recomendaciones breves, claras y relevantes. Si no hay suficiente\n",
    "      información externa, escribir \"No consta\".\n",
    "\n",
    "    Formato de salida (JSON válido, sin texto adicional):\n",
    "    {{\n",
    "      \"checklist\": [\"Documento 1\", \"Documento 2\", \"...\"],\n",
    "      \"terminos_clave\": [\"Término: definición breve\", \"Término: definición breve\", \"Término: definición breve\"],\n",
    "      \"objetivo_y_buenas_practicas\": [\"Buena práctica: detalle breve\", \"Buena práctica: detalle breve\", \"Buena práctica: detalle breve\"],\n",
    "      \"cierre\": \"Fuente: base de conocimiento vigente\"\n",
    "    }}\n",
    "\n",
    "    Reglas generales:\n",
    "    - No inventes información normativa en el checklist: debe provenir de <<BASE>>.\n",
    "    - El JSON debe ser la única salida (no agregues texto fuera del objeto).\n",
    "    - Mantener redacción clara y precisa.\n",
    "\n",
    "    Pregunta: \"{pregunta}\"\n",
    "\n",
    "    <<BASE>>\n",
    "    {base}\n",
    "    <<FIN_BASE>>\n",
    "    \"\"\".strip()\n",
    "ejecutar_prompt(construir_prompt_v5, fila_sel, consulta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d311292f",
   "metadata": {},
   "source": [
    "### Versión 6 – Prompt optimizado en costos (Fast Prompting)\n",
    "\n",
    "En esta última versión, el objetivo principal es **optimizar el uso de tokens** para reducir los costos de cada llamada al modelo, manteniendo el control sobre la estructura de salida.\n",
    "\n",
    "**Características principales:**\n",
    "- **Prompt más breve (lean prompting):** reglas condensadas y redactadas con frases cortas.  \n",
    "- **BASE compacta:** se limita el texto normativo a un máximo de caracteres, eliminando redundancias y evitando información superflua.  \n",
    "- **Formato en JSON conciso:** salida estructurada, fácil de validar y con menor extensión que una redacción en prosa.  \n",
    "- **Ámbito mixto:**  \n",
    "  - *Checklist* → restringido a la BASE (máxima confiabilidad normativa).  \n",
    "  - *Términos clave* y *Buenas prácticas* → permitidos con conocimiento externo (para enriquecer la explicación).  \n",
    "\n",
    "**Técnicas de *Fast Prompting* aplicadas:**\n",
    "- **Delimitadores (`<<BASE>> … <<FIN_BASE>>`)** → aíslan con claridad el contenido normativo.  \n",
    "- **Output en JSON** → controla la extensión y facilita la validación automática.  \n",
    "- **Rol + reglas explícitas pero compactas** → reducen tokens innecesarios sin perder claridad.  \n",
    "- **Self-contained instruction** → cada instrucción está contenida en el mismo bloque, evitando dependencias externas y asegurando consistencia.  \n",
    "\n",
    "Este diseño demuestra cómo el *Fast Prompting* no solo impacta en la **calidad de la respuesta**, sino también en la **eficiencia económica** del sistema: en las pruebas, la Versión 6 logró mantener la riqueza informativa de la Versión 5 pero reduciendo tokens y alcanzando un costo promedio cercano a **0.00020 USD por consulta**, lo que equivale a apenas **0.02 USD por 100 consultas semanales**.  \n",
    "Así, se alcanza un equilibrio entre **precisión normativa, claridad pedagógica y optimización de recursos** en escenarios de producción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a0937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RESPUESTA DEL MODELO ---\n",
      "\n",
      "{\n",
      "  \"checklist\": [\n",
      "    \"Credencial o DNI del afiliado/a titular\",\n",
      "    \"DNI del menor o constancia de parto donde figuren madre y padre\",\n",
      "    \"Ficha de afiliación cumplimentada (no certificada, solo la parte del afiliado)\",\n",
      "    \"Último recibo de haberes\",\n",
      "    \"CUIL si lo posee\",\n",
      "    \"Certificado de nacimiento del menor\"\n",
      "  ],\n",
      "  \"terminos_clave\": [\n",
      "    \"DNI: Documento Nacional de Identidad, identificación oficial en Argentina.\",\n",
      "    \"CUIL: Código Único de Identificación Laboral, número que identifica a los trabajadores en el sistema de seguridad social.\",\n",
      "    \"Constancia de parto: Documento que acredita el nacimiento de un menor, indicando los datos de los padres.\"\n",
      "  ],\n",
      "  \"objetivo_y_buenas_practicas\": [\n",
      "    \"Asegurar la correcta afiliación del recién nacido para garantizar el acceso a la salud.\",\n",
      "    \"Mantener la documentación actualizada y completa para evitar inconvenientes en el proceso de afiliación.\",\n",
      "    \"Realizar la afiliación lo antes posible tras el nacimiento para asegurar la cobertura desde el inicio.\"\n",
      "  ],\n",
      "  \"cierre\": \"Fuente: base de\n",
      "\n",
      "--- COSTOS ---\n",
      "Modelo: gpt-4o-mini | Temp: 0.1 | Máx. tokens salida: 250\n",
      "Prompt tokens: 336 | Completion tokens: 250 | Total: 586\n",
      "Costo entrada (USD): 0.00005040\n",
      "Costo salida  (USD): 0.00015000\n",
      "Costo total (USD): 0.00020040\n"
     ]
    }
   ],
   "source": [
    "def compactar_texto(s: str, max_chars=800):\n",
    "    \"\"\"Compacta espacios y recorta la BASE a un máximo de caracteres.\"\"\"\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s[:max_chars]\n",
    "\n",
    "def construir_prompt_v6(fila, pregunta: str):\n",
    "    base = (fila.get(\"respuesta_validada\") or \n",
    "            fila.get(\"contenido\") or \n",
    "            fila.get(\"titulo\",\"\")).strip()\n",
    "    base = compactar_texto(base, 800)  # compactar para ahorrar tokens\n",
    "\n",
    "    return f\"\"\"\n",
    "    Rol: Asistente de Afiliaciones de IOMA. Público: agentes. Tono: institucional.\n",
    "\n",
    "    Instrucciones:\n",
    "    - \"checklist\": SOLO de <<BASE>>. Si falta dato: \"No consta en la normativa adjunta\".\n",
    "    - \"terminos_clave\" y \"objetivo_y_buenas_practicas\": podés usar conocimiento externo.\n",
    "    - Responder SOLO con JSON válido.\n",
    "\n",
    "    Formato:\n",
    "    {{\n",
    "      \"checklist\": [\"...\", \"...\"],\n",
    "      \"terminos_clave\": [\"Término: definición breve\", \"...\", \"...\"],\n",
    "      \"objetivo_y_buenas_practicas\": [\"Buena práctica: detalle breve\", \"...\", \"...\"],\n",
    "      \"cierre\": \"Fuente: base de conocimiento vigente\"\n",
    "    }}\n",
    "\n",
    "    Pregunta: \"{pregunta}\"\n",
    "\n",
    "    <<BASE>>\n",
    "    {base}\n",
    "    <<FIN_BASE>>\n",
    "    \"\"\".strip()\n",
    "_ = ejecutar_prompt(\n",
    "    construir_prompt_v6,\n",
    "    fila_sel,\n",
    "    consulta,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,   # baja para consistencia\n",
    "    max_tokens=250     # JSON conciso, menor costo\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889db6ea",
   "metadata": {},
   "source": [
    "## Conclusiones finales\n",
    "\n",
    "A lo largo de las seis versiones se observa una clara evolución tanto en la **calidad estructural de las respuestas** como en la **eficiencia económica**.  \n",
    "La siguiente tabla resume el costo por consulta y la proyección semanal para 100 consultas:\n",
    "\n",
    "| Versión | Prompt tokens | Completion tokens | Total tokens | Costo por consulta (USD) | Costo semanal (100 consultas) (USD) |\n",
    "|---------|---------------|-------------------|--------------|---------------------------|--------------------------------------|\n",
    "| V1 | 86  | 66  | 152 | 0.00005250 | 0.00525 |\n",
    "| V2 | 124 | 42  | 166 | 0.00004380 | 0.00438 |\n",
    "| V3 | 181 | 85  | 266 | 0.00007815 | 0.00782 |\n",
    "| V4 | 427 | 342 | 769 | 0.00026925 | 0.02693 |\n",
    "| V5 | 482 | 252 | 734 | 0.00022350 | 0.02235 |\n",
    "| V6 | 336 | 250 | 586 | 0.00020040 | 0.02004 |\n",
    "\n",
    "**Análisis:**\n",
    "\n",
    "- **Versión 1 y 2:** son las más simples y de menor costo, pero con respuestas poco estructuradas y menos útiles en un entorno real.  \n",
    "- **Versión 3:** introduce reglas básicas y estructura, lo que mejora la claridad, aunque incrementa el costo.  \n",
    "- **Versión 4:** aporta mayor control con subsecciones y extensión limitada, pero al estar cerrada exclusivamente a la BASE genera la respuesta más costosa (≈0.027 USD/semana por 100 consultas).  \n",
    "- **Versión 5:** incorpora el modelo mixto (BASE para checklist y externo para definiciones/buenas prácticas). Se logra una respuesta más pedagógica, con costo menor que V4 pero aún alto respecto a las primeras.  \n",
    "- **Versión 6:** aplica técnicas de *Fast Prompting* (BASE compacta, reglas breves, JSON conciso). Mantiene la riqueza de V5 pero optimiza tokens, reduciendo el costo semanal a **≈0.020 USD por 100 consultas**.  \n",
    "\n",
    "**Balance general:**  \n",
    "El análisis muestra que un diseño de prompt más estructurado implica **más tokens y mayor costo**, pero que con optimizaciones de *Fast Prompting* (V6) es posible alcanzar un equilibrio: respuestas claras, normativamente correctas y con bajo costo operativo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
